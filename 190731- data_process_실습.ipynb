{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_process_실습_마지막 시간.ipynb의 사본","version":"0.3.2","provenance":[{"file_id":"10UO9-7l8meTXGK_CpYD1Qfi68J6eAd5g","timestamp":1564562515066}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Lw6aNxDXKNzq","colab_type":"text"},"source":["###load_data() 는 현재 tf.data로 구성되어 있음 -> 이것을 tf.placeholder로 받아오는 예제로 변경하는 소스 코드 작성\n","\n","1) tf.placeholders 각 inputs, labels, lengths에 대해서 만들기 \\\n","2) train.inputs, label.inputs 파일 읽어서 [[문장1, 문장2,...]], [[문장1_label, 문장2_label,...]] 리스트 만들기\\\n","3) 문장 리스트 내에 있는 문장 1 : [단어1, 단어2,...] 를 모두 단어 id로 변경 -> make_vocab_table()의 word2id dictionary를 사용할 수 있음\n","4) get_batch_data() 함수 작성 : training example에서 batch별로 데이터 받아오는 함수\\\n","5) get_bastch_data내에 rank2pad process를 통해 batch 내 문장들을 padding 처리해줘야 함, label 데이터에 대해서도 padding 처리 진행\\\n","6) train.model() 부분의 sess.run 부분에 batch_data 받은 것 각각 feed_dict 해주기\\\n","7) build_model 부분에서 label에 대한 masking 하는 소스 추가\\\n","8) 학습!"]},{"cell_type":"code","metadata":{"id":"SxOzA9RsBDOV","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4wTGh_1BAbl","colab_type":"code","colab":{}},"source":["# placeholders\n","inputs = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"inputs_ph\")\n","lenghts = tf.placeholder(shape=[None], dtype=tf.int32, name=\"lengths_ph\")\n","\n","# 둘 중에 선택\n","# Masking 필요한 label format [3, 10] -> label에도 padding을 붙임\n","labels = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"labels_ph\")\n","\n","# Masking 필요 없는 label format -> [21] -> label에 padding 안 붙이고 shape을 rank 1로 설정 (data_process 쪽에서의 처리가 필요)\n","labels = tf.placeholder(shape=[None], dtype=tf.int32, name=\"labels_ph\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_CHruZKsIeX4","colab_type":"text"},"source":["## Padding 처리 되어 있지 않은 리스트 padding 처리 하기\n","### return 값은 rank 2짜리 paddding 처리된 리스트 -> np.array로 만들 수도 있음"]},{"cell_type":"code","metadata":{"id":"ktrRvn3zAynv","colab_type":"code","colab":{}},"source":["# list inputs!\n","# e.g. [[1,40,2,1],[5,38],[3,12,30,10,3,600,2]...] word를 id로 바꾼 문장 리스트 (각각이 문장)\n","def rank_2_pad_process(inputs, special_id=False):\n","\tappend_id = 0\n","\tif special_id:\n","\t\tappend_id = -1 # user_id -> -1\n","\n","\tmax_sent_len = 0\n","\tfor sent in inputs:\n","\t\tmax_sent_len = max(len(sent), max_sent_len)\n","\n","\t# print(\"text_a max_lengths in a batch\", max_sent_len)\n","\tpadded_result = []\n","\tsent_buffer = []\n","\tfor sent in inputs:\n","\t\tfor i in range(max_sent_len - len(sent)):\n","\t\t\tsent_buffer.append(append_id)\n","\t\tsent.extend(sent_buffer)\n","\t\tpadded_result.append(sent)\n","\t\tsent_buffer = []\n","    \n","    #padded_result = np.array(padded_result)\n","    \n","\treturn padded_result\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V3-iCI0FH565","colab_type":"text"},"source":["## File 읽어서 문장, label 별로 list 만들기"]},{"cell_type":"code","metadata":{"id":"9zzxkuJvCIJ-","colab_type":"code","colab":{}},"source":["import os\n","file_path = \"./CoNLL-2003\"\n","\n","def get_training_examples():\n","  _read_data(os.path.join(\"train.inputs\"))\n","\n","  with open(os.path.join(file_path, \"train.inputs\"), \"r\", encoding=\"utf-8\") as f_handle:\n","  train_data = [for line in f_handle if len(line.rstrip()) > 0]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjwHnBe6JEgT","colab_type":"code","colab":{}},"source":["class InputExample(object):\n","\t\"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","\tdef __init__(self, input_text, label):\n","\t\tself.input_text = input_text\n","\t\tself.label = label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCYqc0s7I7_6","colab_type":"code","colab":{}},"source":["\tdef _read_data(self, data_dir, shuffle=False):\n","\t\tprint(\"[Reading %s]\" % data_dir)\n","\t\twith open(data_dir, \"r\", encoding=\"utf-8\") as fr_handle:\n","\t\t\ttotal_data = [line.strip() for line in fr_handle if len(line.strip()) > 1]\n","      \n","      split_data_list = []\n","      for data in total_data:\n","        split_data_list.append(data.split(\" \"))\n","\n","        if shuffle and self.hparams.training_shuffle_num > 1:\n","\t\t\ttotal_data = self._data_shuffling(total_data, self.hparams.training_shuffle_num)\n","\n","\t\treturn total_data\n","\n","\tdef _create_examples(self, inputs, labels, set_type):\n","\t\t\"\"\"Creates examples for the training and dev sets.\"\"\"\n","\t\texamples = []\n","\t\tfor (i, input_data) in enumerate(inputs):\n","\t\t\t\n","\n","\t\t\texamples.append(\n","\t\t\t\tInputExample(input_text=input_text, label=labels))\n","\t\tprint(\"%s data creation is finished! %d\" % (set_type, len(examples)))\n","\n","\t\treturn examples"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yOeMLWsSIXr3","colab_type":"text"},"source":["## 데이터 Batch 별로 읽어올 수 있는 부분"]},{"cell_type":"code","metadata":{"id":"XoeoZgpYBCFX","colab_type":"code","colab":{}},"source":["\n","\tdef get_batch_data(self, curr_index, batch_size, set_type=\"train\"):\n","\t\tinputs = []\n","\t\tlengths = []\n","\t\tlabel_ids = []\n","\n","\t\texamples = {\n","\t\t\t\"train\": self.train_example,\n","\t\t\t\"test\": self.test_example\n","\t\t}\n","\t\texample = examples[set_type]\n","\n","\t\tfor index, each_example in enumerate(example[curr_index * batch_size:batch_size * (curr_index + 1)]):\n","\t\t\ttokenized_inputs_id, input_length, label_id = \\\n","\t\t\t\tconvert_single_example(each_example, self.label_list, word2id=self.word2id)\n","\n","\t\t\tinputs.append(tokenized_inputs_id)\n","\t\t\tlengths.append(input_length)\n","\t\t\tlabel_ids.append(label_id)\n","\n","\t\tpad_inputs = rank_2_pad_process(inputs)\n","\n","\t\treturn pad_inputs, lengths, label_ids"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kGmqWdfoIKZk","colab_type":"text"},"source":["## 추후에 Glove pre-trained embedding을 가져다 쓸 때 필요한 부분"]},{"cell_type":"code","metadata":{"id":"giAtnzuvIH2e","colab_type":"code","colab":{}},"source":["def get_word_embeddings(self):\n","  with np.load(self.hparams.glove_embedding_path) as data:\n","    print(\"glove embedding shape\", np.shape(data[\"embeddings\"]))\n","    return data[\"embeddings\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYLoKn8GIU2Q","colab_type":"code","colab":{}},"source":["def export_trimmed_glove_vectors():\n","\tglove_embedding = dict()\n","\tglove_vocab = set()\n","\n","\tprint(\"Making glove trimmed.npz, It will take few minutes...\")\n","\twith open(os.path.join(glove_dir, \"glove.6B.300d.txt\"), \"r\", encoding='utf-8') as f_handle:\n","\t\tfor line_idx, line in enumerate(f_handle):\n","\t\t\tline = line.strip().split(\" \")\n","\t\t\tglove_word = line[0]\n","\t\t\tglove_vocab.add(glove_word)\n","\t\t\tglove_embedding[glove_word] = [float(x) for x in line[1:]]\n","\n","\t\tprint(\"# glove vocab : \", len(glove_embedding))\n","\n","\twith open(os.path.join(data_dir, \"vocab.txt\"), \"r\") as f_handle:\n","\t\ttotal_vocab = [line.strip() for line in list(f_handle)]\n","\t\tprint(\"# total vocab : \", len(total_vocab))\n","\n","\t# print(\"%d words are in Glove...\" % len(glove_vocab.intersection(set(total_vocab))))\n","\n","\tembeddings = np.zeros([len(total_vocab) + 1, embedding_dim])\n","\n","\tfor word_idx, word in enumerate(total_vocab):\n","\t\ttry:\n","\t\t\tembeddings[word_idx] = np.asarray(glove_embedding[word.lower()])\n","\t\texcept KeyError:\n","\t\t\tembeddings[word_idx] = np.random.uniform(-1, 1, embedding_dim)\n","\n","\tnp.savez_compressed(os.path.join(glove_dir, \"glove.6B.300d.trimmed.npz\"), embeddings=embeddings)\n","\tprint(\"Save Glove Embeddings as an Numpy Array : \", len(embeddings))\n"],"execution_count":0,"outputs":[]}]}